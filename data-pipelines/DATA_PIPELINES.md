# Data Pipelines
This section explains the data pipelines used in the project - Data Generation Pipeline and Data Ingestion Pipeline. It also covers the end-to-end workflow, from deploying code to triggering the pipelines using Cloud Run.

## Overview
Both DAGs are deployed on a VM on GCP. [Deployment details link]
- A GitHub workflow is used to deploy the code to the VM on push.
- The Data Generation DAG loads preprocessed data and saves newly generated data in FirestoreDB.
- A Cloud Run function is set up to detect any changes in FirestoreDB.
- When a change is detected, the Cloud Run function triggers the Data Ingestion Pipeline.

For details on setting up the required GCP resources to run these pipelines, refer to [GCP Setup](/docs/DATA_PIPELINES_GCP_Setup.md)

## Data Generation Pipeline

### Overview
- The DAG is deployed on a VM running on port 8080.
- Using only the job dataset, this DAG generates data for posts and user profiles, then loads them into FirestoreDB.
- It utilizes the preprocessed and versioned data uploaded to a GCP bucket, as explained in [DATA_PREPROCESS.md](DATA_PREPROCESS.md).

### Tasks:

- `check_file_exists`: Verifies if the preprocessed file is available at the specified path in the GCP bucket.
- `load_jobs`: Validates and loads all job data into the database in batches.

- `create_recruiter_posts`: Leverages basic details from the preprocessed dataset (company name, job description, job title) to generate recruiter profiles and corresponding hiring posts.

- `create_interview_exp_posts`: Uses company name and job title from the dataset to create user profiles and generate posts about their interview experiences for specific roles and companies.

### Visualization

#### Pipeline Diagram: 

#### Gantt Chart:

### LLM Utilization

- We use the OpenRouter API via the LangChain OpenAI package to generate text-based content.
- The model ensures structured responses validated using Pydantic.
- Model Used: meta-llama/llama-3.3-70b-instruct:free
- OpenRouter Models: https://openrouter.ai/models

## Testing and Validation

### Pydantic Validation

All Pydantic validation classes are stored in the `schema/` directory. These classes ensure data integrity before inserting records into Firestore DB.
Validation ensures:
  - Correct data types.
  - Required fields are present.
  - Structured output for LLM-generated content.

Key Pydantic Classes:
- `BasicUser`: Validates user data generated by the LLM.

- `User`: Ensures the correct schema for user data before loading into Firestore.

- `UserList`: Validates bulk user data uploads.

- `JobPosting`: Validates job posting data before adding to Firestore.

- `JobPostingList`: Ensures integrity for lists of job postings.

- `Post`: Validates a post object before loading into Firestore.

- `LinkedInPost`: Ensures structured content for LLM-generated posts.

Additionally, supporting Enums and dependent classes are used to enforce structure and consistency for outputs.

### Testing
1. Type Conversion & Cleaning: Ensuring numeric/string company IDs are standardized, NaN handling, and dtype consistency.
2. Schema Enforcement: Validation of JobPosting Pydantic models with edge cases like epoch timestamp handling.
3. Data Integrity: Testing dataframe operations (merges, drops, enrichment) in enrich_and_clean_postings to ensure valid titles, locations, and company mappings.
4. Company-User Mapping: Verifying correct user allocation ratios with create_company_user_map.
5. ID Generation Workflows: Testing UUID generation, rate limiting, and uniqueness checks for user/post creation.
6. External Service Simulation: Mocking LLM chains (e.g., DummyPostChain, DummyUserChain) to validate structured JSON outputs without real API calls.
7. Rate Limiter Behavior: Simulating allowed/blocked states to test conditional logic in data generation pipelines.
8. Deterministic UUIDs: Using patched UUIDs to verify ID assignment and relational integrity between users and posts.
9. Empty/missing data, invalid types, and schema violations (e.g., testing ValidationError for malformed job postings).
10. Stress-testing functions with mixed data types, zero values, and unexpected NaN propagation.

Test suite combines pandas-based assertions, Pydantic model validation, and unittest.mock to isolate components, ensuring reliability across data pipelines, model hydration, and synthetic data generation workflows.

The results of a test run for all the files in data-generation/dags/src/

![alt text](/images/test-cases.png)


## Cloud Run Function
Once the Data Generation Pipeline is triggered and data is loaded into FirestoreDB, a Cloud Function hosted on Cloud Run is configured to detect any additions or updates in FirestoreDB. This triggers the Data Ingestion Pipeline accordingly. 

[[Setup](../cloud-functions/functions/dag-trigger/README.md)]

[[Function](../cloud-functions/functions/dag-trigger/dag-firebase-trigger.py)]


## Data Ingestion Piepeline

### Overview
- This pipeline loads data from FirestoreDB into the Pinecone Vector Store.
- The DAG is deployed on a VM and is accessible via port 9090. 
- A GitHub workflow updates the code on the VM whenever thereâ€™s a push. - The pipeline is automatically triggered by Cloud Run.

### Tasks
- `initialize_pinecone`: Initializes the Pinecone index. If the index doesn't exist, it will be created.

- `test_connections`: Verifies connectivity and functionality for the following services:

  1. Firestore: Ensures Firestore is accessible.

  2. Pinecone: Confirms the availability of the vector database.

  3. Embedding Model: Verifies that the embedding model is working correctly.

- `ingest_data`: Ingests data into the Pinecone vector store, processing one namespace at a time.

### Visualization

#### Pipeline Diagram: 
![Data Ingestion Pipeline](/images/data_ingestion.png)

#### Gantt Chart:
![Data Ingestion Gantt Chart](/images/data-ingestion-gantt.png)

### Testing
[TODO]

## Logging and Failure Tracking
- Logs are generated at each step and for all functions.
- The logs are currently collected using the in-built Airflow logger
- Errors are captured and logged for easy debugging and resolution.
- Any errors during the pipeline runs result in a failed run, and appropriate notifications are sent to alert about the status.

## Alerting

All DAGs send an email notification updating the status. The email notifications are sent for both failed/successful runs of the data pipelines. An example of the email notification is attached below.

![Notification Email Example](/images/image_10.png)

## GitHub Workflows
There are two workflows triggered on a push to the main branch. These workflows perform the following tasks:

- SSH into the VM.

- Pull the latest code from the repository.

- Ensure that all required files, such as the .env file and GCP credentials JSON, are available. If not, they are added.

- Restart the Docker containers and ensure the Airflow endpoint is up and running.

The workflow definitions can be found in the corresponding YAML files:
- [data-generation-workflow](../.github/workflows/trigger_airflow_generation.yml)
- [data-generation-workflow](../.github/workflows/trigger_airflow_data_pipeline.yml)
