# Data Pipelines
This section explains the data pipelines used in the project - Data Generation Pipeline and Data Ingestion Pipeline. It also covers the end-to-end workflow, from deploying code to triggering the pipelines using Cloud Run.

## Overview
Both DAGs are deployed on a VM on GCP. [Deployment details link]
- A GitHub workflow is used to deploy the code to the VM on push.
- The Data Generation DAG loads preprocessed data and saves newly generated data in FirestoreDB.
- A Cloud Run function is set up to detect any changes in FirestoreDB.
- When a change is detected, the Cloud Run function triggers the Data Ingestion Pipeline.


## Prerequisites & Related Docs
- [Data Exploration](/docs/DATA_EXPLORE.md)
- [Data Preprocessing](/docs/DATA_PREPROCESS.md)
- [GCP Setup](/docs/GCP_SETUP.md)
- [Pinecone & Docker Setup](/docs/ESSENTIAL_SERVICES_SETUP.md)


## Data Generation Pipeline

### Overview
- The DAG is deployed on a VM running on port 8080.
- Using only the job dataset, this DAG generates data for posts and user profiles, then loads them into FirestoreDB.
- It utilizes the preprocessed and versioned data uploaded to a GCP bucket, as explained in [DATA_PREPROCESS.md](DATA_PREPROCESS.md).

### Tasks:

- `check_file_exists`: Verifies if the preprocessed file is available at the specified path in the GCP bucket.
- `load_jobs`: Validates and loads all job data into the database in batches.

- `create_recruiter_posts`: Leverages basic details from the preprocessed dataset (company name, job description, job title) to generate recruiter profiles and corresponding hiring posts.

- `create_interview_exp_posts`: Uses company name and job title from the dataset to create user profiles and generate posts about their interview experiences for specific roles and companies.

*Note: Prompts for generating data are in `prompts.py` file*

### Visualization

#### Pipeline Diagram: 
![Data Generation Pipeline](/images/data-generation-pipeline.png)
#### Gantt Chart:
![Data Generation Gantt Chart](/images/data-generation-gantt.png)
### LLM Utilization

- We use the OpenRouter API via the LangChain OpenAI package to generate text-based content.
- The model ensures structured responses validated using Pydantic.
- Model Used: meta-llama/llama-3.3-70b-instruct:free
- OpenRouter Models: https://openrouter.ai/models

## Testing and Validation

### Pydantic Validation

All Pydantic validation classes are stored in the `schema/` directory. These classes ensure data integrity before inserting records into Firestore DB.
Validation ensures:
  - Correct data types.
  - Required fields are present.
  - Structured output for LLM-generated content.

Key Pydantic Classes:
- `BasicUser`: Validates user data generated by the LLM.

- `User`: Ensures the correct schema for user data before loading into Firestore.

- `UserList`: Validates bulk user data uploads.

- `JobPosting`: Validates job posting data before adding to Firestore.

- `JobPostingList`: Ensures integrity for lists of job postings.

- `Post`: Validates a post object before loading into Firestore.

- `LinkedInPost`: Ensures structured content for LLM-generated posts.

Additionally, supporting Enums and dependent classes are used to enforce structure and consistency for outputs.

### Testing
1. Type Conversion & Cleaning: Ensuring numeric/string company IDs are standardized, NaN handling, and dtype consistency.
2. Schema Enforcement: Validation of JobPosting Pydantic models with edge cases like epoch timestamp handling.
3. Data Integrity: Testing dataframe operations (merges, drops, enrichment) in enrich_and_clean_postings to ensure valid titles, locations, and company mappings.
4. Company-User Mapping: Verifying correct user allocation ratios with create_company_user_map.
5. ID Generation Workflows: Testing UUID generation, rate limiting, and uniqueness checks for user/post creation.
6. External Service Simulation: Mocking LLM chains (e.g., DummyPostChain, DummyUserChain) to validate structured JSON outputs without real API calls.
7. Rate Limiter Behavior: Simulating allowed/blocked states to test conditional logic in data generation pipelines.
8. Deterministic UUIDs: Using patched UUIDs to verify ID assignment and relational integrity between users and posts.
9. Empty/missing data, invalid types, and schema violations (e.g., testing ValidationError for malformed job postings).
10. Stress-testing functions with mixed data types, zero values, and unexpected NaN propagation.

Test suite combines pandas-based assertions, Pydantic model validation, and unittest.mock to isolate components, ensuring reliability across data pipelines, model hydration, and synthetic data generation workflows.

The results of a test run for all the files in data-generation/dags/src/

![alt text](/images/test-cases.png)


## Cloud Run Function
Once the Data Generation Pipeline is triggered and data is loaded into FirestoreDB, a Cloud Function hosted on Cloud Run is configured to detect any additions or updates in FirestoreDB. This triggers the Data Ingestion Pipeline accordingly. 

[[Setup](../infra/functions/dag-trigger/README.md)]

[[Function](../infra/functions/dag-trigger/dag-firebase-trigger.py)]


## Data Ingestion Piepeline

### Overview
- This pipeline loads data from FirestoreDB into the Pinecone Vector Store.
- The DAG is deployed on a VM and is accessible via port 9090. 
- A GitHub workflow updates the code on the VM whenever thereâ€™s a push. - The pipeline is automatically triggered by Cloud Run.

### Tasks
- `initialize_pinecone`: Initializes the Pinecone index. If the index doesn't exist, it will be created.

- `test_connections`: Verifies connectivity and functionality for the following services:

  1. Firestore: Ensures Firestore is accessible.

  2. Pinecone: Confirms the availability of the vector database.

  3. Embedding Model: Verifies that the embedding model is working correctly.

- `ingest_data`: Ingests data into the Pinecone vector store, processing one namespace at a time.

### Visualization

#### Pipeline Diagram: 
![Data Ingestion Pipeline](/images/data_ingestion.png)

#### Gantt Chart:
![Data Ingestion Gantt Chart](/images/data-ingestion-gantt.png)

### Testing
[TODO]

## Logging and Failure Tracking
- Logs are generated at each step and for all functions.
- The logs are currently collected using the in-built Airflow logger
- Errors are captured and logged for easy debugging and resolution.
- Any errors during the pipeline runs result in a failed run, and appropriate notifications are sent to alert about the status.

## Alerting

All DAGs send an email notification updating the status. The email notifications are sent for both failed/successful runs of the data pipelines. An example of the email notification is attached below.

![Notification Email Example](/images/image_10.png)

## GitHub Workflows

Two GitHub Actions workflows are configured to ensure that the latest code is always available on the VMs hosting the data pipelines. These workflows are triggered on every push to the main branch and handle syncing code to the VM and restarting relevant services.

For more details on each workflow, refer to the [CI/CD Workflows](/docs/CI_CD_Workflows.md) README.


### Trigger DAGs Locally
Ensure Docker and Docker Compose is setup. Follow instructions [here](/docs/ESSENTIAL_SERVICES_SETUP.md)

Navigate to the DAGs directory:
```bash
cd data-pipelines/data-generation/dags
```
or
```bash
cd data-pipelines/data-ingestion/dags
```

Initialize Airflow(first time)
```bash
docker compose up airflow-init
```

Run
```bash
docker compose up
```
Once running, visit http://localhost:8080 (or your configured port) to access the Airflow UI.

Stop docker container
```bash
docker compose down
```
